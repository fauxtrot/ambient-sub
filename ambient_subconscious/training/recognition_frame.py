"""Recognition Frame - Output schema for infant model predictions"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
import torch
import json
from datetime import datetime
import uuid


@dataclass
class RecognitionFrame:
    """
    Recognition frame generated by infant model.

    Starts simple (Stage 0: has_sound) and grows progressively.
    For Stage 0 implementation, only has_sound is used.
    """

    # Stage 0 (Infant) - Binary sound detection
    has_sound: bool

    # Stage 1+ fields (not used in Stage 0)
    has_speaker: Optional[bool] = None
    speaker_changed: Optional[bool] = None
    speaker_id: Optional[int] = None
    num_speakers: Optional[int] = None

    # Temporal features (Stage 3+)
    when_started: Optional[float] = None
    how_long: Optional[float] = None
    silence_duration: Optional[float] = None

    # Rich recognition (Stage 4+)
    what: Optional[str] = None
    confidence: float = 0.0

    # Input features and metadata
    encodec_tokens: Optional[List[int]] = None
    where: str = "unknown"  # Source symbol
    timestamp: float = 0.0
    frame_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    session_id: str = ""

    # Temporal context (for self-feeding)
    previous_frames: Optional[List["RecognitionFrame"]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary (for JSON serialization)"""
        return {
            "has_sound": self.has_sound,
            "has_speaker": self.has_speaker,
            "speaker_changed": self.speaker_changed,
            "speaker_id": self.speaker_id,
            "num_speakers": self.num_speakers,
            "when_started": self.when_started,
            "how_long": self.how_long,
            "silence_duration": self.silence_duration,
            "what": self.what,
            "confidence": self.confidence,
            "where": self.where,
            "timestamp": self.timestamp,
            "frame_id": self.frame_id,
            "session_id": self.session_id,
        }

    def to_json(self) -> str:
        """Serialize to JSON string"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "RecognitionFrame":
        """Create from dictionary"""
        return cls(
            has_sound=data["has_sound"],
            has_speaker=data.get("has_speaker"),
            speaker_changed=data.get("speaker_changed"),
            speaker_id=data.get("speaker_id"),
            num_speakers=data.get("num_speakers"),
            when_started=data.get("when_started"),
            how_long=data.get("how_long"),
            silence_duration=data.get("silence_duration"),
            what=data.get("what"),
            confidence=data.get("confidence", 0.0),
            where=data.get("where", "unknown"),
            timestamp=data.get("timestamp", 0.0),
            frame_id=data.get("frame_id", str(uuid.uuid4())[:8]),
            session_id=data.get("session_id", ""),
        )

    @classmethod
    def from_json(cls, json_str: str) -> "RecognitionFrame":
        """Deserialize from JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_tensor(self, stage: int = 0) -> torch.Tensor:
        """
        Convert to tensor for model input/output.

        For Stage 0: Just binary has_sound as [1.0] or [0.0]
        """
        if stage == 0:
            return torch.tensor([1.0 if self.has_sound else 0.0])

        # Future stages would include more features
        features = [
            1.0 if self.has_sound else 0.0,
            1.0 if self.has_speaker else 0.0,
            1.0 if self.speaker_changed else 0.0,
            float(self.speaker_id) if self.speaker_id is not None else -1.0,
            self.confidence,
        ]
        return torch.tensor(features)

    @classmethod
    def from_tensor(cls, tensor: torch.Tensor, stage: int = 0, **kwargs) -> "RecognitionFrame":
        """Create from model output tensor"""
        if stage == 0:
            # Stage 0: Single value for has_sound
            has_sound = tensor[0].item() > 0.5
            return cls(has_sound=has_sound, **kwargs)

        # Future stages
        return cls(
            has_sound=tensor[0].item() > 0.5,
            has_speaker=tensor[1].item() > 0.5 if len(tensor) > 1 else None,
            **kwargs
        )
